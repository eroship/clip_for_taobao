{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git config --global http.sslverify \"false\"\n!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T03:53:54.662215Z","iopub.execute_input":"2022-06-25T03:53:54.662578Z","iopub.status.idle":"2022-06-25T03:54:16.349315Z","shell.execute_reply.started":"2022-06-25T03:53:54.662532Z","shell.execute_reply":"2022-06-25T03:54:16.348172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport clip\nimport json\nfrom PIL import Image\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport numpy as np\nfrom re import split\nimport pandas as pd\nimport csv","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:54:16.352402Z","iopub.execute_input":"2022-06-25T03:54:16.352747Z","iopub.status.idle":"2022-06-25T03:54:16.358802Z","shell.execute_reply.started":"2022-06-25T03:54:16.352719Z","shell.execute_reply":"2022-06-25T03:54:16.357795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Tokenizer(object):\n    def __init__(self, emb_path=None, json_path=None):\n        self.standard_color = ['红','黄','蓝','绿','橙','青','紫','白','灰','黑','粉','橘','银','金','栗','驼','咖','卡其','杏','棕','米','靛','褐']\n        #用来在词语中搜索\n        self.standard_color2 = ['红色', '黄色', '蓝色', '绿色', '橙色', '青色', '紫色', '白色', '灰色', '黑色', '粉色','橘色','银色','金色','栗色','驼色','咖色','卡其色','杏色','浅杏色','棕色','米色','靛','靛蓝','肉色','褐色']\n        #用来在emb_table中搜索\n        self.emb_path = emb_path\n        self.json_path = json_path\n        self.tag_english_dict = {}\n        self.standard_tag_list = []\n        #print('finish reading embedding table, num of emb_words: ', len(self.emb_words))\n\n    def __len__(self):\n        return len(self.tag_english_dict)\n\n    def __call__(self, tag):\n        return self.tag_english_dict.get(tag, 'None')\n\n    #读取json文件，构造三个dict\n    # 图片文件名-颜色标签对应的dict，颜色标签-标号dict，标号-向量dict\n    def if_tag_exist(self, tag):\n        if np.where(np.array(self.standard_tag_list)==tag)[0].__len__() > 0:\n            return True\n        else:\n            return False\n\n\n    def build_tag_english_dict(self, tag_english_path, standard_tag_txt_path):\n        standard_tags = pd.read_csv(standard_tag_txt_path, header=None, encoding='utf-8', sep=' ', quoting=csv.QUOTE_NONE,\n                                error_bad_lines=False)\n        english_tags = pd.read_csv(tag_english_path, header=None, encoding='utf-8', sep='\\n', quoting=csv.QUOTE_NONE,\n                                error_bad_lines=False)\n        self.standard_tag_list = []\n        standard_tags = np.array(standard_tags[0])\n        english_tags = np.array(english_tags[0])\n        #(standard_tags.__len__())\n        #print(english_tags.__len__())\n        #print()\n        for standard_tag,english_tag in zip(standard_tags, english_tags):\n            self.tag_english_dict[standard_tag] = english_tag.lower()\n            self.standard_tag_list.append(standard_tag)\n\n    def generate_standard_tag_txt(self, standard_tag_txt_path):\n        #生成一个包含所有标准tag的txt文件，用作翻译\n        emb_table = pd.read_csv(self.emb_path, header=None, encoding='utf-8', sep=' ', quoting=csv.QUOTE_NONE,\n                                error_bad_lines=False)\n        standard_tags = np.array(emb_table[0])\n        for standard_tag in standard_tags:\n            if len(standard_tag) == 1:\n                standard_tag = standard_tag+'色'\n            self.standard_tag_list.append(standard_tag)\n\n        self.standard_tag_list = set(self.standard_tag_list)\n        for standard_tag in self.standard_tag_list:\n            with open(standard_tag_txt_path, 'a', encoding='utf-8') as f:\n                f.write(standard_tag + '\\n')\n\n\n\n    def get_color_emb_table(self, standard_tags_list_duplicate, save_path):\n        #原始数据集的emb_table太大了，只需要取出所有可能用到的颜色的词语然后保存\n        emb_path = self.emb_path\n        print('num of tags: ', len(standard_tags_list_duplicate))\n        names = os.listdir(emb_path)\n        tag_not_matched = []\n        for i, standard_tag in enumerate(standard_tags_list_duplicate):\n            print('tag： ',i,' ',standard_tag)\n            ifFound = False\n            for name in names:\n                path = emb_path+name\n                emb_table = pd.read_csv(path, header=None, encoding='utf-8', sep=' ', quoting=csv.QUOTE_NONE,\n                                        error_bad_lines=False)\n                words = np.array(emb_table[0])\n                loc = np.where(words == standard_tag)[0]\n                if len(loc)!=0:\n                    loc = loc[0]\n                    print(emb_table.iloc[loc])\n                    emb_list = list(emb_table.iloc[loc])\n                    for t,val in enumerate(emb_list):\n                        emb_list[t] = str(val)\n\n                    emb_str = ' '.join(emb_list)\n                    print(emb_str)\n                    with open(save_path, 'a', encoding='utf-8') as f:\n                        f.write(emb_str +' '+'\\n')\n\n                    ifFound = True\n                    break\n\n            if ifFound == False:\n                tag_not_matched.append(standard_tag)\n        self.emb_path = save_path\n        return tag_not_matched\n\n    def get_standard_tag_from_json(self):\n        #读取json文件，整理出所有图片对应的标准标签\n        json_path = self.json_path\n        standard_tags_list = []\n        with open(json_path, 'rt', encoding='utf-8') as f:\n            img_tag_dict = json.load(f)\n        for name, data in img_tag_dict.items():\n            img_tags = data['imgs_tags']\n            for img_tag in img_tags:\n                imgname = list(img_tag.keys())[0]\n                label = img_tag[imgname]\n                standard_tag = standardize(label)\n                standard_tags_list.append(standard_tag)\n                if len(standard_tag)==1:\n                    standard_tags_list.append(standard_tag+'色')\n        return standard_tags_list","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:54:16.360624Z","iopub.execute_input":"2022-06-25T03:54:16.361585Z","iopub.status.idle":"2022-06-25T03:54:16.390277Z","shell.execute_reply.started":"2022-06-25T03:54:16.361533Z","shell.execute_reply":"2022-06-25T03:54:16.38913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def standardize(tag):\n    #去除所有括号、数字、字母、横斜杠、加减号\n    #直接在括号处截断，带来的问题：【白色】单件衬衫，这样的就没了\n    #所以后续需要在标准色库中再搜索一遍\n    sp = '\\(|\\[|【|{|（|<|《| '\n    tag = split(sp,tag)[0]\n    tag = tag.split('+')[0]\n\n    char_to_delete = []\n    for idx, c in enumerate(tag):\n        asc = ord(c)\n        if asc<128:\n            tag = tag.replace(c, '')\n\n    if tag.count('色')>0:\n        tag = tag.split('色')[0] + '色'\n\n    return tag","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:54:16.393911Z","iopub.execute_input":"2022-06-25T03:54:16.394196Z","iopub.status.idle":"2022-06-25T03:54:16.402525Z","shell.execute_reply.started":"2022-06-25T03:54:16.394172Z","shell.execute_reply":"2022-06-25T03:54:16.401598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class mydataset(Dataset):\n    def __init__(self, data_dir, train, tokenizer, preprocess=None):\n        self.tokenizer = tokenizer\n        self.data_dir = data_dir\n        self.train = train\n        self.data_path = Path(data_dir)/'train_all.json' if train else Path(data_dir)/'test_all.json'\n        self.imgname_list = []\n\n        self.img_tag_list = []\n        self.img_tag_english_list = []\n        self.img_optionaltags_english_list = []\n        self.img_optionaltags_list = []\n\n        self.preprocess = preprocess\n\n        self.dataset_init()\n\n    def __len__(self):\n        return len(self.imgname_list)\n\n    def __getitem__(self, item):\n        imgname = self.imgname_list[item]\n        if self.train == True:\n            img_path = self.data_dir + '/train/' + imgname.split('_')[0] + '/' + imgname\n            img = Image.open(img_path)\n            tag_english = self.img_tag_english_list[item]\n            tag_english = 'A photo of a '+tag_english+' color cloth'\n            #print(tag_english)\n            img = self.preprocess(img)\n            text = clip.tokenize(tag_english).squeeze()\n            return img, text\n        else:\n            img_path = self.data_dir + '/test/' + imgname.split('_')[0] + '/' + imgname\n            #print(img_path)\n            img = Image.open(img_path)\n            #optional_tags = self.img_optionaltags_dict[imgname]\n            optional_tags_english = self.img_optionaltags_english_list[item]\n            return img, optional_tags_english\n\n    '''def get(self, imgname):\n        if self.train == True:\n            img_path = self.data_dir + '/train/' + imgname.split('_')[0] + '/' + imgname\n            img = Image.open(img_path)\n            tag_english = self.img_tag_english_dict[imgname]\n            return img, tag_english\n        else:\n            img_path = self.data_dir + '/test/' + imgname.split('_')[0] + '/' + imgname\n            #print(img_path)\n            img = Image.open(img_path)\n            #optional_tags = self.img_optionaltags_dict[imgname]\n            optional_tags_english = self.img_optionaltags_english_dict[imgname]\n            return img, optional_tags_english'''\n\n    def dataset_init(self):\n        with open(self.data_path, 'rt', encoding='utf-8') as f:\n            img_tag_dict = json.load(f)\n        for name, data in img_tag_dict.items():\n            img_tags = data['imgs_tags']\n            optional_tags = data['optional_tags']\n            #print(optional_tags)\n            optional_tags_english = []\n            for optional_tag in optional_tags:\n                opt_tag = self.get_tag_for_label(optional_tag)\n                optional_tags_english.append(self.tokenizer(opt_tag))\n\n\n            for img_tag in img_tags:\n                imgname = list(img_tag.keys())[0]\n                self.imgname_list.append(imgname)\n                self.img_optionaltags_list.append(optional_tags)\n                self.img_optionaltags_english_list.append(optional_tags_english)\n\n                if self.train == True:\n                    label = img_tag[imgname]\n                    tag = self.get_tag_for_label(label)\n                    self.img_tag_english_list.append(self.tokenizer(tag))\n                    self.img_tag_list.append(tag)\n\n    def get_tag_for_label(self, label):\n        tag = standardize(label)\n        #print('standard: ',tag)\n        if len(tag) == 1:\n            tag = tag + '色'\n        if self.tokenizer.if_tag_exist(tag) == False:\n            if_found = False\n            for std_color in self.tokenizer.standard_color:\n                if label.count(std_color) > 0:\n                    if_found = True\n                    tag = std_color + '色'\n                    break\n            if if_found == False:\n                tag = '<BOS>'\n        return tag","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:54:16.403993Z","iopub.execute_input":"2022-06-25T03:54:16.404511Z","iopub.status.idle":"2022-06-25T03:54:16.423417Z","shell.execute_reply.started":"2022-06-25T03:54:16.404476Z","shell.execute_reply":"2022-06-25T03:54:16.422541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epochs, dataset, batchsize, model, device, optimizer):\n    batch_num = len(dataset)/batchsize\n    print(device)\n    train_dataloader = DataLoader(dataset, batchsize)\n    loss_img = nn.CrossEntropyLoss()\n    loss_text = nn.CrossEntropyLoss()\n    \n    model = model.to(device)\n    loss_img = loss_img.to(device)\n    loss_text = loss_text.to(device)\n\n    for epoch in range(1,epochs+1):\n        n = 1\n        for img, text in train_dataloader:\n            #print(img)\n            #print(img.shape)\n            #print(text)\n            #print(text.shape)\n            optimizer.zero_grad()\n            img = img.to(device)\n            text = text.to(device)\n            logits_per_image, logits_per_text = model(img, text)\n            ground_truth = torch.arange(len(img), dtype=torch.long, device=device)\n\n            total_loss = (loss_img(logits_per_image, ground_truth) + loss_text(logits_per_text, ground_truth))/2\n            total_loss.backward()\n            optimizer.step()\n            \n            if n%10==0:\n                print('epoch: ', epoch, ' batch: ',n,' total: ', batch_num, ' loss: ', total_loss.item())\n            n=n+1\n            \n        if epoch%3==0:\n            torch.save({\n                'epoch':epoch,\n                'model_state_dict':model.state_dict(),\n                'optimizer_state_dict':optimizer.state_dict(),\n                'loss':total_loss,\n            }, './'+str(epoch)+'.pt')  \n            print('succesfully save')\n\n        #print('epoch: ',epoch, ' loss: ', total_loss.item())\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:15:52.454515Z","iopub.execute_input":"2022-06-25T04:15:52.455497Z","iopub.status.idle":"2022-06-25T04:15:52.467778Z","shell.execute_reply.started":"2022-06-25T04:15:52.455452Z","shell.execute_reply":"2022-06-25T04:15:52.466765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(name, device):\n    model_clip = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n    for mc in model_clip:\n        if name == mc:\n            model, preprocess = clip.load(name, device, jit=False)\n            optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n            return model, preprocess, optimizer\n\n    model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)\n    checkpoint = torch.load(name)\n\n    #checkpoint['model_state_dict'][\"input_resolution\"] = 224#input_resolution\n    #checkpoint['model_state_dict'][\"context_length\"] = 77 #model.context_length\n    #checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.02)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    print('successfully load ', name)\n\n    return model, preprocess, optimizer","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:16:04.916753Z","iopub.execute_input":"2022-06-25T04:16:04.917385Z","iopub.status.idle":"2022-06-25T04:16:04.925037Z","shell.execute_reply.started":"2022-06-25T04:16:04.917351Z","shell.execute_reply":"2022-06-25T04:16:04.923649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.cuda.set_device(0)\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nstandard_tag_path = '../input/tag-txt/standard_tags.txt'\ntag_english_path = '../input/tag-txt/standard_tags_english.txt'\ntokenizer = Tokenizer()\ntokenizer.build_tag_english_dict(tag_english_path, standard_tag_path)\nprint(len(tokenizer))\n\n#model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\nmodel, preprocess, optimizer = get_model('../input/model-pretrain/24.pt', device)\nprint('successfully load')\n\ndata_dir = '../input/dataset-img-tag/thumbnail'\ndataset_train = mydataset(data_dir, True, tokenizer, preprocess)\nprint(len(dataset_train.img_tag_english_list))\nprint(len(dataset_train.imgname_list))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:06:10.618488Z","iopub.execute_input":"2022-06-25T04:06:10.619167Z","iopub.status.idle":"2022-06-25T04:07:07.101949Z","shell.execute_reply.started":"2022-06-25T04:06:10.619131Z","shell.execute_reply":"2022-06-25T04:07:07.100063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch=21\nbatchsize = 128\ntrain(epoch, dataset_train, batchsize, model, device, optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:16:13.009657Z","iopub.execute_input":"2022-06-25T04:16:13.010558Z","iopub.status.idle":"2022-06-25T04:16:52.612903Z","shell.execute_reply.started":"2022-06-25T04:16:13.010519Z","shell.execute_reply":"2022-06-25T04:16:52.610612Z"},"trusted":true},"execution_count":null,"outputs":[]}]}